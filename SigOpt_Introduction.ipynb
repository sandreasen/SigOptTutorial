{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SigOpt for TensorFlow Introduction\n",
    "\n",
    ">*This notebook is created by Tobias Andreasen. For any comments, suggestions etc., please contact TobiasAndreasen1992@gmail.com. *\n",
    "\n",
    "The **Black Box Optimization Tool** SigOpt (https://sigopt.com/) allows you to do easy **Hyperparameter tuning**, which in most cases helps improving performance of various models.\n",
    "\n",
    "SigOpt is a black box optimization tool in the sense that it runs completly on The SigOpt backend **WITHOUT** ever seeing the actual data. What SigOpt requires is a set of hyperparameters and one or two matricies on which you want to maximize performance.\n",
    "\n",
    ">*Please see https://sigopt.com/ for questions regarding SigOpt licensing *\n",
    "\n",
    ">* The two classification examples (a simple first and a more complex second) in this notebook are based on examples from https://www.tensorflow.org/tutorials/image_recognition to build on the tensorflow tutorial exposure that tensorflow users would already have. The use of the examples is under the creative commons attribution 3.0 license which can be found at https://creativecommons.org/licenses/by/3.0/legalcode or in human readable format at https://creativecommons.org/licenses/by/3.0/*\n",
    "\n",
    "\n",
    "In this notebook, I employ the above classificaiton examples as a tutorial vehicle to introduce some of the features that SigOpt provides for working in a Python enviroment. For simplicity  we will be using the MNIST-dataset. The MNIST-dataset comprises roughly 60,000 images of handwriten digits, which we want to classify. See the pictures later in the notebook. Interested users can have fun with substituting the notMNINST dataset at https://github.com/davidflanagan/notMNIST-to-MNIST for the MINST dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import requests\n",
    "from ipywidgets import Image\n",
    "\n",
    "import sigopt\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def plot_images(images, title, imgsize):\n",
    "    n = images.shape[0]\n",
    "    n_cols = 10\n",
    "    n_rows = ceil(n / n_cols)\n",
    "    plt.figure(figsize=(1.8 * n_cols, 2 * n_rows))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plot_image(ax, images[i,:], imgsize)\n",
    "        \n",
    "def plot_image(ax, image, imgsize):\n",
    "    plt.imshow(image.reshape(imgsize), cmap='Greys_r')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SigOpt for TensorFlow Introduction\n",
    "\n",
    ">*This notebook is created by Tobias Andreasen. For any comments, suggestions ect please contact TobiasAndreasen1992@gmail.com for any question regarding license please see https://sigopt.com/*\n",
    "\n",
    "\n",
    "The **Black Box Optimization Tool** SigOpt (https://sigopt.com/) allows you to do easy **Hyperparameter tuning**, which in most cases helps improving performance of various models.\n",
    "\n",
    "SigOpt is a black box optimization tool in the sense that it runs completly on The SigOpt backend **WITHOUT** ever seeing the actual data. What SigOpt requires is a set of hyperparameters and one or two matricies on which you want to maximize performance.\n",
    "\n",
    "The following will introduce two hands-on examples created in TensorFlow (https://www.tensorflow.org/), where I introduce some of the features that SigOpt provides working in a Python enviroment. For simplicity we will be looking at two classification problems, and for that we will be using the MNIST-dataset. The MNIST-dataset comprises roughly 60.000 images of handwriten digits, which we want to classify. See the pictures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABAEAAACBCAYAAACxfKPTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164659e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(mnist.train.images[:10,:], \"Train examples extract\", (28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SigOpt differs from most open-source tools due to the fact that it uses a trade-off between Exploitation and Exploration.\n",
    "Most open-source tools are only interested in optimizing the first found optimum through a series of iterations without even considering the potential of additional optimums.\n",
    "SigOpt outperforms these tools by having the ability to jump between optimums, because it tries to maximize the information it recives after every given iteration.\n",
    "\n",
    "For more information I can recommand the following podcast: https://soundcloud.com/twiml/twiml-talk-050-bayesian-optimization-hyperparameter-tuning-scott-clark\n",
    "\n",
    "\n",
    ">SigOpt automates the tuning of your model’s ***hyper, feature, and architecture parameters.*** If you’re not optimizing them, ***you’re forsaking significant performance and revenue gains.***\n",
    "\n",
    ">Modelers often overlook these optimizations because traditional approaches like manual, grid, and random search are time-consuming and produce subpar results.\n",
    "\n",
    ">Let SigOpt modernize your workflow so you can focus on what you’re best at: designing your model and understanding your data. <cite> https://sigopt.com/ <cite>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Simple Neural Network\n",
    "\n",
    "The first example is an MNIST Image classification problem using a 2-layer neural network architecture:\n",
    "\n",
    "**input $\\rightarrow$ fully connected $\\rightarrow$ activation $\\rightarrow$ fully connected $\\rightarrow$ output**\n",
    "\n",
    "For this example we introduce 5 hyperparameters, that we want to use in order to increase performance of the model.\n",
    "\n",
    "For using SigOpt, the first step is to create a connection to the SigOpt backend, by either using the API Token or the Development Token: https://sigopt.com/user/profile\n",
    "\n",
    "***The Development Token is for debugging and DOES NOT count as an experiment.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc1aea91f4c484fa4af458bf1431e2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Image(value=requests.get('https://www.tensorflow.org/images/softmax-regression-scalargraph.png'\n",
    "                        ).content, width=300, height=150)\n",
    "\n",
    "# If it does not complie, run:\n",
    "# jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sigopt.Connection(client_token=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly we need to choose an observation budget. The observation budget corresponds to the number of iterations that we want to provide to SigOpt. SigOpt suggests to use a budget in the range of 10-20 times the number of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation_budget_experiment_1 = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create an experiment via the SigOpt API, but this can also be done using Python. When creating the experiment, the following pieces of information need to be specified:\n",
    "\n",
    "* Name of the experiment\n",
    "* Hyperparameters, including spans\n",
    "* Metrics/objective\n",
    "* Observation Budget\n",
    "\n",
    "Choosing the hyperparameter spans is based on knowlegde of the person implementing the model, ***but throughout the experiment these can be changed using the SigOpt API.***\n",
    "\n",
    "It is possible to choose three types of Hyperparameters:\n",
    "\n",
    "1. Floating point values\n",
    "2. Categorical values\n",
    "3. Integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiments = conn.experiments().create(\n",
    "    name = \"SigOpt_Workshop_Example_1\",\n",
    "        parameters = [\n",
    "            \n",
    "          # DOUBLE\n",
    "          {'name': 'lr',         'type': 'double', 'bounds': {'min': 5e-9, 'max': 5e-1}}, # learning rate\n",
    "            \n",
    "          # Categorical\n",
    "          {'name': 'activation', 'type': 'categorical', 'categorical_values': ['softmax', 'relu', 'softplus']}, # activation\n",
    "          \n",
    "          # Integer  \n",
    "          {'name': 'nodes',      'type': 'int', 'bounds': {'min': 50, 'max': 200}}, # nodes in second hiddden layer\n",
    "          {'name': 'iterations', 'type': 'int', 'bounds': {'min': 100, 'max': 1500}}, # number of training iterations\n",
    "          {'name': 'batch_size', 'type': 'int', 'bounds': {'min': 1, 'max': 200}}, # batch size\n",
    "          \n",
    "        ], metrics=[ dict(name=\"accuracy\") ],\n",
    "      observation_budget = observation_budget_experiment_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an experiment has been created, the experiment ID has to be retrived. It can also be accessed via the SigOpt API: https://sigopt.com/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28839\n"
     ]
    }
   ],
   "source": [
    "ID = experiments.id\n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create a loop that trains the model a number of times corresponding to the observation budget. Every iteration will then consist of four steps:\n",
    "\n",
    "1. Get Hyperparameter suggestion from SigOpt\n",
    "2. Generat model with the given hyperparameters\n",
    "3. Train and evaluate the model\n",
    "4. Return information to Sigopt\n",
    "\n",
    "While this is running the results can be monitored on: https://sigopt.com/experiments \n",
    "\n",
    "***It is worth noticing that this is a toy example, and that the performance can be increased by increasing the number of training iterations.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    for _ in range(observation_budget):\n",
    "\n",
    "        \n",
    "        \"\"\"Get Hyperparameter suggestion from SigOpt\"\"\"\n",
    "        suggestion = conn.experiments(ID).suggestions().create()\n",
    "        params = suggestion.assignments\n",
    "\n",
    "        activation = params['activation']\n",
    "        nodes = params['nodes']\n",
    "        lr = params['lr']\n",
    "        iterations = params['iterations']\n",
    "        batch_size = params['batch_size']\n",
    "\n",
    "        \n",
    "        \"\"\"Generat model with the given Hyperparameter\"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        with tf.name_scope('scope'):\n",
    "            W_1 = tf.Variable(tf.zeros([784, nodes]), name='W_1')\n",
    "            b_1 = tf.Variable(tf.zeros([nodes]), name='b_1')\n",
    "\n",
    "            W_2 = tf.Variable(tf.zeros([nodes, 10]), name='W_2')\n",
    "            b_2 = tf.Variable(tf.zeros([10]), name='b_2')\n",
    "\n",
    "        z = getattr(tf.nn, activation)(tf.matmul(x, W_1) + b_1)\n",
    "        y = tf.nn.softmax(tf.matmul(z, W_2) + b_2)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        \n",
    "        \"\"\"Train and evaluate the model\"\"\"\n",
    "        for i in range(iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "        \n",
    "        \n",
    "        \"\"\"Return information to Sigopt\"\"\"\n",
    "        conn.experiments(ID).observations().create(\n",
    "        suggestion=suggestion.id,\n",
    "        value=float(result)) # if only one objective we return using value\n",
    "\n",
    "        \n",
    "except:\n",
    "    KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have shown that our basic setup works, and we are ready to dig a little bit deeper into what SigOpt has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example - Deep Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second example we are still looking at the MNIST dataset but this time we're choosing a deeper architecture. We do this for two reasons. First, we want to include more hyperparameters and secondly this type model also happen to be very multimodal; meaning that they have multiple optimums. The model architecture chosen is pictured below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088b64d36fe245bdbe9354d25ee737c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Image(value=requests.get('https://www.tensorflow.org/images/mnist_deep.png').content, width=300, height=600)\n",
    "\n",
    "# If it does not complie, run:\n",
    "# jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def unpack_hp(params):\n",
    "    return (params['f_1'], params['f_2'], params['k_1'], params['k_2'],\n",
    "           params['a_1'], params['a_2'], params['o_s'], params['do'],\n",
    "           params['lr'], params['opt'], params['b_s'], params['i'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second experiment we are interested in investigating if the model we are looking at has multiple optimums with equavalent performance.\n",
    "\n",
    "Together with the increase in the number of hyperparameters, we decide to also increase our observation budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation_budget_experiment_2 = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we are including 12 hyperparameters, where 2 are floating points, 3 are categorical and 7 are integers. Choosing a large number of categorical values can sometimes be problematic, since the SigOpt-model has to search through all the vertices of a hypercube of size $g^{k}$, where $g$ is the number of potential hyperparameter-values and $k$ is the number of categorical hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.0000001 # The SigOpt parameter spans are inclusive, so due to the numerical instability of\n",
    "                    # some hyperparameters, we include a small epsilon.\n",
    "\n",
    "experiments = conn.experiments().create(\n",
    "    name = \"SigOpt_Workshop_Example_2\",\n",
    "        parameters = [\n",
    "            \n",
    "          # DOUBLE\n",
    "          {'name': 'lr', 'type': 'double', 'bounds': {'min': 5e-9, 'max': 5e-1}}, # learning rate\n",
    "          {'name': 'do', 'type': 'double', 'bounds': {'min': 0+epsilon, 'max': 1-epsilon}}, # dropout rate\n",
    "            \n",
    "          # Categorical\n",
    "          {'name': 'a_1', 'type': 'categorical', 'categorical_values': ['softmax', 'relu', 'softplus']}, # activation 1\n",
    "          {'name': 'a_2', 'type': 'categorical', 'categorical_values': ['softmax', 'relu', 'softplus']}, # activations 2\n",
    "          {'name': 'opt', 'type': 'categorical', 'categorical_values': ['AdamOptimizer', 'AdadeltaOptimizer']}, # optimizer\n",
    "            \n",
    "          # Integer  \n",
    "          {'name': 'f_1', 'type': 'int', 'bounds': {'min': 1, 'max':75}}, # numbers of filters in first layer\n",
    "          {'name': 'f_2', 'type': 'int', 'bounds': {'min': 1, 'max': 75}}, # numbers of filters in second layer\n",
    "          {'name': 'k_1', 'type': 'int', 'bounds': {'min': 2, 'max': 10}}, # kernel size in first layer\n",
    "          {'name': 'k_2', 'type': 'int', 'bounds': {'min': 2, 'max': 10}}, # kernel size in second layer\n",
    "          {'name': 'o_s', 'type': 'int', 'bounds': {'min': 500, 'max': 2500}}, # size of first fully connected layer \n",
    "          {'name': 'b_s', 'type': 'int', 'bounds': {'min': 1, 'max': 100}}, # batch size  \n",
    "          {'name': 'i',   'type': 'int', 'bounds': {'min': 50, 'max': 1000}}, # number of training iterations  \n",
    "            \n",
    "        ], metrics=[ {'name': 'accuracy'}, {'name': 'train_time'} ],\n",
    "      observation_budget = observation_budget_experiment_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are assuming that the model we are working with has multiple optimums, we will try to find a good trade-off between the accuracy of the model and the time it takes to train the model.\n",
    "To do so we will be maximizing over the accuracy but also the negative of the training time.\n",
    "\n",
    "The remaining code follows the procedure from the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28840\n"
     ]
    }
   ],
   "source": [
    "ID = experiments.id\n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Again, it is worth noticing that this is a toy example, and that the performance can be increased by increasing the number of training iterations.***\n",
    "\n",
    "Increasing the number of training iterations to 20.000 gives an accuracy of 99.2% but take roughly half an hour to train on a normal CPU.\n",
    "\n",
    "By optimizing over the accuracy as well as the negative training time, we want to see if we can hit that performance at a much lower training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    for _ in range(observation_budget):\n",
    "        \n",
    "\n",
    "        \"\"\"Get Hyperparameter suggestion from SigOpt\"\"\"\n",
    "        suggestion = conn.experiments(ID).suggestions().create()\n",
    "        params = suggestion.assignments\n",
    "\n",
    "        # Hyperparameters\n",
    "        num_filters_1, num_filters_2, size_kernel_1, size_kernel_2,\\\n",
    "        activation_1, activation_2, output_size, dropout, learning_rate,\\\n",
    "        optim_func, batch_size, iterations = unpack_hp(params)\n",
    "        \n",
    "        \n",
    "        \"\"\"Generat model with the given Hyperparameter\"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # input\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "        # Layer 1\n",
    "        W_conv1 = weight_variable([size_kernel_1,\n",
    "                                   size_kernel_1,\n",
    "                                   1,\n",
    "                                   num_filters_1])\n",
    "\n",
    "        b_conv1 = bias_variable([num_filters_1])\n",
    "\n",
    "        h_conv1 = getattr(tf.nn, activation_1)(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "        # Layer 2\n",
    "        W_conv2 = weight_variable([size_kernel_2,\n",
    "                                   size_kernel_2,\n",
    "                                   num_filters_1,\n",
    "                                   num_filters_2])\n",
    "\n",
    "        b_conv2 = bias_variable([num_filters_2])\n",
    "\n",
    "\n",
    "        h_conv2 = getattr(tf.nn, activation_2)(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "        # fully connected 1\n",
    "        W_fc1 = weight_variable([int(h_pool2.shape[1])**2 * num_filters_2, output_size])\n",
    "        b_fc1 = bias_variable([output_size])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, int(h_pool2.shape[1])**2 * num_filters_2])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "        # fully connected 2\n",
    "        W_fc2 = weight_variable([output_size, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "\n",
    "\n",
    "        # output layer\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "\n",
    "        train_step = getattr(tf.train, optim_func)(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        time_start = time.time()\n",
    "        \n",
    "        \n",
    "        \"\"\"Train and evaluate the model\"\"\"\n",
    "        for i in range(iterations):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: dropout})\n",
    "\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # Evaluate model\n",
    "        acc = accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "        train_time = time_start-time_end # we want to maximize the negative time in order to reduce it\n",
    "\n",
    "        \n",
    "        \"\"\"Return information to Sigopt\"\"\"\n",
    "        result = [{'name': 'accuracy', 'value': float(acc)},\n",
    "                  {'name': 'train_time', 'value': float(train_time)}]\n",
    "\n",
    "        conn.experiments(ID).observations().create(\n",
    "        suggestion=suggestion.id,\n",
    "        values=result) # if two objectives we return using values\n",
    "\n",
    "        \n",
    "except:\n",
    "    KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this example we have tried to update a model with competing objectives. For doing this type of optimization it important to become familiar with the term **Pareto Efficiency** (https://en.wikipedia.org/wiki/Pareto_efficiency). The following blogpost gives an additional intuition: https://blog.sigopt.com/posts/intro-to-multicriteria-optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
